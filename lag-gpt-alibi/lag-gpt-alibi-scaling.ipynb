{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from itertools import islice\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from estimator import LagGPTAlibiEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDatasetIterator:\n",
    "    def __init__(self, datasets, seed, weights):\n",
    "        self._datasets = [iter(el) for el in datasets]\n",
    "        self._weights = weights\n",
    "        self._rng = random.Random(seed)\n",
    "\n",
    "    def __next__(self):\n",
    "        (dataset,) = self._rng.choices(self._datasets, weights=self._weights, k=1)\n",
    "        return next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset:\n",
    "    def __init__(self, datasets, seed=None, weights=None):\n",
    "        self._seed = seed\n",
    "        self._datasets = datasets\n",
    "        self._weights = weights\n",
    "        n_datasets = len(datasets)\n",
    "        if weights is None:\n",
    "            self._weights = [1 / n_datasets] * n_datasets\n",
    "\n",
    "    def __iter__(self):\n",
    "        return CombinedDatasetIterator(self._datasets, self._seed, self._weights)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([len(ds) for ds in self._datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out most datasets for testing\n",
    "\n",
    "gluonts_ds = [\n",
    "        get_dataset(\"airpassengers\").train,\n",
    "        # get_dataset(\"australian_electricity_demand\").train,\n",
    "        # get_dataset(\"car_parts_without_missing\").train,\n",
    "        # get_dataset(\"cif_2016\").train,\n",
    "        # get_dataset(\"covid_deaths\").train,\n",
    "        # get_dataset(\"electricity\").train,\n",
    "        # get_dataset(\"electricity_weekly\").train,\n",
    "        # get_dataset(\"exchange_rate\").train,\n",
    "        # get_dataset(\"fred_md\").train,\n",
    "        # get_dataset(\"hospital\").train,\n",
    "        # get_dataset(\"kaggle_web_traffic_weekly\").train,\n",
    "        # get_dataset(\"kdd_cup_2018_without_missing\").train,\n",
    "        # get_dataset(\"london_smart_meters_without_missing\").train,\n",
    "        # get_dataset(\"nn5_daily_with_missing\").train,\n",
    "        # get_dataset(\"nn5_weekly\").train,\n",
    "        # get_dataset(\"pedestrian_counts\").train,\n",
    "        # get_dataset(\"rideshare_without_missing\").train,\n",
    "        # get_dataset(\"saugeenday\").train,\n",
    "        # get_dataset(\"solar-energy\").train,\n",
    "        # get_dataset(\"solar_10_minutes\").train,\n",
    "        # get_dataset(\"solar_weekly\").train,\n",
    "        # get_dataset(\"taxi_30min\").train,\n",
    "        # get_dataset(\"temperature_rain_without_missing\").train,\n",
    "        # get_dataset(\"tourism_monthly\").train,\n",
    "        # get_dataset(\"uber_tlc_daily\").train,\n",
    "        # get_dataset(\"uber_tlc_hourly\").train,\n",
    "        # get_dataset(\"vehicle_trips_without_missing\").train,\n",
    "        # get_dataset(\"weather\").train,\n",
    "        # get_dataset(\"wiki-rolling_nips\").train,\n",
    "        # get_dataset(\"m4_daily\").train,\n",
    "        # get_dataset(\"m4_hourly\").train,\n",
    "        # get_dataset(\"m4_monthly\").train,\n",
    "        # get_dataset(\"m4_quarterly\").train,\n",
    "        # get_dataset(\"m4_yearly\").train,\n",
    "        # get_dataset(\"wind_farms_without_missing\").train,\n",
    "]\n",
    "\n",
    "# Training daaset\n",
    "dataset = CombinedDataset(gluonts_ds, weights=[sum([len(x[\"target\"]) for x in d]) for d in gluonts_ds])\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = get_dataset(\"m4_weekly\").test\n",
    "\n",
    "# Obtain prediction_length out of dataset metadata\n",
    "meta = get_dataset(\"m4_weekly\").metadata\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = dict(max_epochs=5, accelerator=\"cpu\", precision=\"16\")\n",
    "# trainer_kwargs = dict(max_epochs=100, accelerator=\"gpu\", precision=\"bf16-mixed\", logger=WandbLogger())\n",
    "\n",
    "estimator = LagGPTAlibiEstimator(\n",
    "    prediction_length=meta.prediction_length,\n",
    "    context_length=1024, # block_size: int = 2048 \n",
    "    batch_size=32, # 4\n",
    "    n_layer=8,\n",
    "    n_head=4,\n",
    "    n_embd=64, # 4096\n",
    "    scaling=\"std\",\n",
    "    num_batches_per_epoch=100,\n",
    "    trainer_kwargs=trainer_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(\n",
    "    training_data=dataset, \n",
    "    validation_data=val_dataset,\n",
    "    shuffle_buffer_length=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(\"traffic\").test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_dataset, predictor=predictor\n",
    ")\n",
    "\n",
    "tss = list(ts_it)\n",
    "forecasts = list(forecast_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "agg_metrics, ts_metrics = evaluator(\n",
    "    iter(tss), iter(forecasts), num_series=len(test_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_metrics.plot(x=\"MSIS\", y=\"MAPE\", kind=\"scatter\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    forecast.plot(color='g')\n",
    "    ts[-3 * 24:][0].plot(label=\"target\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
