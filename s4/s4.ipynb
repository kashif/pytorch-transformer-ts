{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61debcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e16862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Iterable, Dict, Any, Tuple\n",
    "from itertools import islice\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import tqdm.auto as tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler, MultiplicativeLR\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from gluonts.core.component import validated\n",
    "from gluonts.dataset.common import Dataset, ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "from gluonts.itertools import Cyclic\n",
    "from gluonts.time_feature import (\n",
    "    TimeFeature,\n",
    "    time_features_from_frequency_str,\n",
    ")\n",
    "from gluonts.torch.modules.loss import DistributionLoss, NegativeLogLikelihood\n",
    "from gluonts.transform import (\n",
    "    Transformation,\n",
    "    Chain,\n",
    "    RemoveFields,\n",
    "    SetField,\n",
    "    AsNumpyArray,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AddAgeFeature,\n",
    "    VstackFeatures,\n",
    "    InstanceSplitter,\n",
    "    ValidationSplitSampler,\n",
    "    TestSplitSampler,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    SelectFields,\n",
    ")\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.torch.model.estimator import PyTorchLightningEstimator\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.torch.distributions import (\n",
    "    DistributionOutput,\n",
    "    StudentTOutput,\n",
    ")\n",
    "from gluonts.torch.util import weighted_average\n",
    "from gluonts.torch.scaler import MeanScaler, NOPScaler\n",
    "from gluonts.torch.modules.feature import FeatureEmbedder\n",
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "from gluonts.dataset.repository.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91ca76-45b9-40df-be1d-3b96dcfa6a8d",
   "metadata": {},
   "source": [
    "#### S4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4 import S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: str,\n",
    "        \n",
    "        context_length: int,\n",
    "        prediction_length: int,\n",
    "        \n",
    "        num_feat_dynamic_real: int,\n",
    "        num_feat_static_real: int,\n",
    "        \n",
    "        num_feat_static_cat: int,\n",
    "        cardinality: List[int],\n",
    "        embedding_dimension: Optional[List[int]] = None,\n",
    "        \n",
    "        input_size: int = 1, # univariate input\n",
    "        \n",
    "        # S4 inputs\n",
    "        d_state: int = 64,\n",
    "        nhead: int = 1, #  channels: can be interpreted as a number of \"heads\"\n",
    "        num_layers: int = 1, # Number of layers\n",
    "        dropout_rate: float = 0.2,\n",
    "        prenorm: bool = False, # Prenorm\n",
    "        activation: str = \"gelu\",  # activation in between SS and FF\n",
    "        postact: str = \"glu\",  # activation after FF\n",
    "        measure: str = \"legs\",\n",
    "        trainable: Optional[Dict[str, bool]] = None,\n",
    "        \n",
    "        distr_output: DistributionOutput = StudentTOutput(),\n",
    "        lags_seq: Optional[List[int]] = None,\n",
    "        scaling: bool = True,\n",
    "        num_parallel_samples: int = 100,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.distr_output = distr_output\n",
    "        self.target_shape = distr_output.event_shape\n",
    "        self.num_feat_dynamic_real = num_feat_dynamic_real\n",
    "        self.num_feat_static_cat = num_feat_static_cat\n",
    "        self.num_feat_static_real = num_feat_static_real\n",
    "        self.embedding_dimension = (\n",
    "            embedding_dimension\n",
    "            if embedding_dimension is not None or cardinality is None\n",
    "            else [min(50, (cat + 1) // 2) for cat in cardinality]\n",
    "        )\n",
    "        self.lags_seq = lags_seq or get_lags_for_frequency(freq_str=freq)\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "        \n",
    "        self.embedder = FeatureEmbedder(\n",
    "            cardinalities=cardinality,\n",
    "            embedding_dims=self.embedding_dimension,\n",
    "        )\n",
    "        if scaling:\n",
    "            self.scaler = MeanScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = NOPScaler(dim=1, keepdim=True)\n",
    "        \n",
    "        self.lagged_s4 = LaggedS4(\n",
    "            input_size=input_size,\n",
    "            features_size=self._number_of_features,\n",
    "            lags_seq=[lag - 1 for lag in self.lags_seq],\n",
    "            \n",
    "            # S4 inputs\n",
    "            d_state=d_state,\n",
    "            channels=nhead,\n",
    "            prenorm=prenorm,\n",
    "            activation=activation,\n",
    "            postact=postact,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            #l_max=self._past_length + self.prediction_length,\n",
    "            measure=measure,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "        \n",
    "        self.param_proj = distr_output.get_args_proj(input_size*len(self.lags_seq) + self._number_of_features)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _number_of_features(self) -> int:\n",
    "        return (\n",
    "            sum(self.embedding_dimension)\n",
    "            + self.num_feat_dynamic_real\n",
    "            + self.num_feat_static_real\n",
    "            + self.input_size  # the log(scale)\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def _past_length(self) -> int:\n",
    "        return self.context_length + max(self.lags_seq)\n",
    "\n",
    "    def unroll_lagged_s4(\n",
    "        self,\n",
    "        feat_static_cat: torch.Tensor,\n",
    "        feat_static_real: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        future_time_feat: Optional[torch.Tensor] = None,\n",
    "        future_target: Optional[torch.Tensor] = None,\n",
    "        state: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[\n",
    "        Tuple[torch.Tensor, ...],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        Tuple[torch.Tensor, torch.Tensor],\n",
    "    ]:\n",
    "        context = past_target[:, -self.context_length :]\n",
    "        observed_context = past_observed_values[:, -self.context_length :]\n",
    "        __, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        prior_input = past_target[:, : -self.context_length] / scale\n",
    "        inputs = (\n",
    "            torch.cat((context, future_target[:, :-1]), dim=1) / scale\n",
    "            if future_target is not None\n",
    "            else context / scale\n",
    "        )\n",
    "\n",
    "        unroll_length = (\n",
    "            self.context_length\n",
    "            if future_target is None\n",
    "            else self.context_length + future_target.shape[1] - 1\n",
    "        )\n",
    "        assert inputs.shape[1] == unroll_length\n",
    "\n",
    "        embedded_cat = self.embedder(feat_static_cat)\n",
    "        log_scale = scale.log() if self.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat(\n",
    "            (embedded_cat, feat_static_real, log_scale),\n",
    "            dim=1,\n",
    "        )\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(\n",
    "            -1, unroll_length, -1\n",
    "        )\n",
    "\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_feat[:, -self.context_length + 1 :, ...],\n",
    "                    future_time_feat,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_time_feat is not None\n",
    "            else past_time_feat[:, -self.context_length + 1 :, ...]\n",
    "        )\n",
    "\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        output, new_state = self.lagged_s4(prior_input, inputs, features, state)\n",
    "\n",
    "        params = self.param_proj(output)\n",
    "        return params, scale, output, static_feat, new_state\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def output_distribution(\n",
    "        self, params, scale=None, trailing_n=None\n",
    "    ) -> torch.distributions.Distribution:\n",
    "        sliced_params = params\n",
    "        if trailing_n is not None:\n",
    "            sliced_params = [p[:, -trailing_n:] for p in params]\n",
    "        return self.distr_output.distribution(sliced_params, scale=scale)\n",
    "\n",
    "    # prediction\n",
    "    def forward(\n",
    "        self,\n",
    "        feat_static_cat: torch.Tensor,\n",
    "        feat_static_real: torch.Tensor,\n",
    "        past_time_feat: torch.Tensor,\n",
    "        past_target: torch.Tensor,\n",
    "        past_observed_values: torch.Tensor,\n",
    "        future_time_feat: torch.Tensor,\n",
    "        num_parallel_samples: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if num_parallel_samples is None:\n",
    "            num_parallel_samples = self.num_parallel_samples\n",
    "            \n",
    "        for layer in self.lagged_s4.s4_layers:\n",
    "            layer.kernel.kernel.setup_step()\n",
    "            default_state = layer.kernel.default_state(*past_target.shape[:1])\n",
    "            \n",
    "        params, scale, _, static_feat, state = self.unroll_lagged_s4(\n",
    "            feat_static_cat,\n",
    "            feat_static_real,\n",
    "            past_time_feat,\n",
    "            past_target,\n",
    "            past_observed_values,\n",
    "            future_time_feat[:, :1],\n",
    "            state=default_state,\n",
    "        )\n",
    "\n",
    "        repeated_scale = scale.repeat_interleave(\n",
    "            repeats=num_parallel_samples, dim=0\n",
    "        )\n",
    "        \n",
    "        repeated_static_feat = static_feat.repeat_interleave(\n",
    "            repeats=num_parallel_samples, dim=0\n",
    "        ).unsqueeze(dim=1)\n",
    "        \n",
    "        repeated_past_target = (\n",
    "            past_target.repeat_interleave(\n",
    "                repeats=num_parallel_samples, dim=0\n",
    "            )\n",
    "            / repeated_scale\n",
    "        )\n",
    "        \n",
    "        repeated_time_feat = future_time_feat.repeat_interleave(\n",
    "            repeats=num_parallel_samples, dim=0\n",
    "        )\n",
    "        \n",
    "        repeated_state = state.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "        \n",
    "        repeated_params = [\n",
    "            s.repeat_interleave(repeats=num_parallel_samples, dim=0)\n",
    "            for s in params\n",
    "        ]\n",
    "        \n",
    "        distr = self.output_distribution(\n",
    "            repeated_params, trailing_n=1, scale=repeated_scale\n",
    "        )\n",
    "        next_sample = distr.sample()\n",
    "        future_samples = [next_sample]\n",
    "        \n",
    "        for k in range(1, self.prediction_length):\n",
    "            scaled_next_sample = next_sample / repeated_scale\n",
    "            next_features = torch.cat(\n",
    "                (repeated_static_feat, repeated_time_feat[:, k : k + 1]),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "            output, repeated_state = self.lagged_s4(\n",
    "                repeated_past_target,\n",
    "                scaled_next_sample,\n",
    "                next_features,\n",
    "                repeated_state,\n",
    "                step=True,\n",
    "            )\n",
    "\n",
    "            repeated_past_target = torch.cat(\n",
    "                (repeated_past_target, scaled_next_sample), dim=1\n",
    "            )\n",
    "\n",
    "            params = self.param_proj(output)\n",
    "            \n",
    "            # hack: sometimes the params ie. output has nans\n",
    "            # replace nans with means of the params...\n",
    "            # params = [p.nan_to_num(nan=p.nanmean(0).item()) for p in params]\n",
    "            \n",
    "            distr = self.output_distribution(params, scale=repeated_scale)\n",
    "            next_sample = distr.sample()\n",
    "            future_samples.append(next_sample)\n",
    "\n",
    "        future_samples_concat = torch.cat(future_samples, dim=1)\n",
    "\n",
    "        return future_samples_concat.reshape(\n",
    "            (-1, self.num_parallel_samples, self.prediction_length)\n",
    "            + self.target_shape,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725eb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaggedS4(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        features_size: int,\n",
    "        lags_seq: List[int],\n",
    "        \n",
    "        #s4 inputs\n",
    "        d_state: int = 64,\n",
    "        channels: int = 1, #  channels: can be interpreted as a number of \"heads\"\n",
    "        num_layers: int = 1, # Number of layers\n",
    "        l_max: int = 1, # max length or 1\n",
    "        dropout: float = 0.2,\n",
    "        prenorm: bool = False, # Prenorm flag\n",
    "        activation: str = \"gelu\",  # activation in between SS and FF\n",
    "        postact: str = \"glu\",  # activation after FF\n",
    "        measure: str = \"fourier\",\n",
    "        trainable: Optional[Dict[str, bool]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.features_size = features_size\n",
    "        self.lags_seq = lags_seq\n",
    "        \n",
    "        d_model = input_size * len(self.lags_seq) + features_size\n",
    "        self.prenorm = prenorm\n",
    "        \n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4(\n",
    "                    d_model=d_model,\n",
    "                    d_state=d_state,\n",
    "                    l_max=l_max,\n",
    "                    channels=channels,\n",
    "                    activation=activation,\n",
    "                    dropout=dropout,\n",
    "                    transposed=False, #[B, T, F]\n",
    "                    postact=postact,\n",
    "                    measure=measure,\n",
    "                    trainable=trainable,\n",
    "                    mode=\"nplr\",\n",
    "                    n_ssm=1,\n",
    "                )\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(nn.Dropout2d(dropout))\n",
    "\n",
    "    def get_lagged_subsequences(\n",
    "        self,\n",
    "        sequence: torch.Tensor,\n",
    "        subsequences_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence : Tensor\n",
    "            the sequence from which lagged subsequences should be extracted.\n",
    "            Shape: (N, T, C).\n",
    "        subsequences_length : int\n",
    "            length of the subsequences to be extracted.\n",
    "        Returns\n",
    "        --------\n",
    "        lagged : Tensor\n",
    "            a tensor of shape (N, S, C, I), where S = subsequences_length and\n",
    "            I = len(indices), containing lagged subsequences. Specifically,\n",
    "            lagged[i, j, :, k] = sequence[i, -indices[k]-S+j, :].\n",
    "        \"\"\"\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = self.lags_seq\n",
    "\n",
    "        assert max(indices) + subsequences_length <= sequence_length, (\n",
    "            f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "            f\"while history length is only {sequence_length}\"\n",
    "        )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "\n",
    "    def _check_shapes(\n",
    "        self,\n",
    "        prior_input: torch.Tensor,\n",
    "        input: torch.Tensor,\n",
    "        features: Optional[torch.Tensor],\n",
    "    ) -> None:\n",
    "        assert len(prior_input.shape) == len(input.shape)\n",
    "        assert (\n",
    "            len(prior_input.shape) == 2 and self.input_size == 1\n",
    "        ) or prior_input.shape[2] == self.input_size\n",
    "        assert (len(input.shape) == 2 and self.input_size == 1) or input.shape[\n",
    "            -1\n",
    "        ] == self.input_size\n",
    "        assert (\n",
    "            features is None or features.shape[2] == self.features_size\n",
    "        ), f\"{features.shape[2]}, expected {self.features_size}\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        prior_input: torch.Tensor,\n",
    "        inputs: torch.Tensor,\n",
    "        features: Optional[torch.Tensor] = None,\n",
    "        state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        step: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        self._check_shapes(prior_input, inputs, features)\n",
    "\n",
    "        sequence = torch.cat((prior_input, inputs), dim=1)\n",
    "        lagged_sequence = self.get_lagged_subsequences(\n",
    "            sequence=sequence,\n",
    "            subsequences_length=inputs.shape[1],\n",
    "        )\n",
    "\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(\n",
    "            lags_shape[0], lags_shape[1], -1\n",
    "        )\n",
    "\n",
    "        if features is None:\n",
    "            s4_input = reshaped_lagged_sequence\n",
    "        else:\n",
    "            s4_input = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        x = s4_input\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z)\n",
    "            \n",
    "            if step:\n",
    "                z, state = layer.step(z.squeeze(), state)\n",
    "                z = z.unsqueeze(1)\n",
    "            else:\n",
    "                # Apply S4 block\n",
    "                z, state = layer(z, state)\n",
    "            \n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6bfbe-a1c0-4634-bc7c-7c6fc57aedf0",
   "metadata": {},
   "source": [
    "#### S4 Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16da5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4LightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: S4Model,\n",
    "        loss: DistributionLoss = NegativeLogLikelihood(),\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-8,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def _compute_loss(self, batch):\n",
    "        feat_static_cat = batch[\"feat_static_cat\"]\n",
    "        feat_static_real = batch[\"feat_static_real\"]\n",
    "        past_time_feat = batch[\"past_time_feat\"]\n",
    "        past_target = batch[\"past_target\"]\n",
    "        future_time_feat = batch[\"future_time_feat\"]\n",
    "        future_target = batch[\"future_target\"]\n",
    "        past_observed_values = batch[\"past_observed_values\"]\n",
    "        future_observed_values = batch[\"future_observed_values\"]\n",
    "\n",
    "        params, scale, _, _, _ = self.model.unroll_lagged_s4(\n",
    "            feat_static_cat,\n",
    "            feat_static_real,\n",
    "            past_time_feat,\n",
    "            past_target,\n",
    "            past_observed_values,\n",
    "            future_time_feat,\n",
    "            future_target,\n",
    "        )\n",
    "        distr = self.model.output_distribution(params, scale)\n",
    "\n",
    "        context_target = past_target[:, -self.model.context_length + 1 :]\n",
    "        target = torch.cat(\n",
    "            (context_target, future_target),\n",
    "            dim=1,\n",
    "        )\n",
    "        loss_values = self.loss(distr, target)\n",
    "\n",
    "        context_observed = past_observed_values[\n",
    "            :, -self.model.context_length + 1 :\n",
    "        ]\n",
    "        observed_values = torch.cat(\n",
    "            (context_observed, future_observed_values), dim=1\n",
    "        )\n",
    "\n",
    "        if len(self.model.target_shape) == 0:\n",
    "            loss_weights = observed_values\n",
    "        else:\n",
    "            loss_weights, _ = observed_values.min(dim=-1, keepdim=False)\n",
    "\n",
    "        return weighted_average(loss_values, weights=loss_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):  # type: ignore\n",
    "        \"\"\"Execute training step\"\"\"\n",
    "        train_loss = self._compute_loss(batch)\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            train_loss,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):  # type: ignore\n",
    "        \"\"\"Execute validation step\"\"\"\n",
    "        with torch.no_grad():\n",
    "            val_loss = self._compute_loss(batch)\n",
    "        self.log(\n",
    "            \"val_loss\", val_loss, on_epoch=True, on_step=False, prog_bar=True\n",
    "        )\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Returns the optimizer to use\"\"\"\n",
    "        return torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86661964-156f-458c-9fe3-d9c8818d973b",
   "metadata": {},
   "source": [
    "####Â S4 Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_INPUT_NAMES = [\n",
    "    \"feat_static_cat\",\n",
    "    \"feat_static_real\",\n",
    "    \"past_time_feat\",\n",
    "    \"past_target\",\n",
    "    \"past_observed_values\",\n",
    "    \"future_time_feat\",\n",
    "]\n",
    "\n",
    "TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "    \"future_target\",\n",
    "    \"future_observed_values\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Estimator(PyTorchLightningEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: str,\n",
    "        prediction_length: int,\n",
    "        context_length: Optional[int] = None,\n",
    "\n",
    "        d_state: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        nhead: int = 2,\n",
    "        prenorm: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        postact: str = \"glu\",\n",
    "        dropout_rate: float = 0.1,\n",
    "        measure: str = \"fourier\",\n",
    "        trainable: Optional[Dict[str, bool]] = None,\n",
    "        \n",
    "        num_feat_dynamic_real: int = 0,\n",
    "        num_feat_static_cat: int = 0,\n",
    "        num_feat_static_real: int = 0,\n",
    "        cardinality: Optional[List[int]] = None,\n",
    "        embedding_dimension: Optional[List[int]] = None,\n",
    "        distr_output: DistributionOutput = StudentTOutput(),\n",
    "        loss: DistributionLoss = NegativeLogLikelihood(),\n",
    "        scaling: bool = True,\n",
    "        lags_seq: Optional[List[int]] = None,\n",
    "        time_features: Optional[List[TimeFeature]] = None,\n",
    "        num_parallel_samples: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        num_batches_per_epoch: int = 50,\n",
    "        trainer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        default_trainer_kwargs = {\n",
    "            \"max_epochs\": 100,\n",
    "            #\"gradient_clip_val\": 10.0,\n",
    "        }\n",
    "        if trainer_kwargs is not None:\n",
    "            default_trainer_kwargs.update(trainer_kwargs)\n",
    "        super().__init__(trainer_kwargs=default_trainer_kwargs)\n",
    "\n",
    "        self.freq = freq\n",
    "        self.context_length = (\n",
    "            context_length if context_length is not None else prediction_length\n",
    "        )\n",
    "        self.prediction_length = prediction_length\n",
    "        self.distr_output = distr_output\n",
    "        self.loss = loss\n",
    "        \n",
    "        self.d_state = d_state\n",
    "        self.num_layers = num_layers\n",
    "        self.nhead = nhead\n",
    "        self.activation = activation\n",
    "        self.prenorm = prenorm\n",
    "        self.postact = postact\n",
    "        self.measure = measure\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_feat_dynamic_real = num_feat_dynamic_real\n",
    "        self.num_feat_static_cat = num_feat_static_cat\n",
    "        self.num_feat_static_real = num_feat_static_real\n",
    "        self.cardinality = (\n",
    "            cardinality if cardinality and num_feat_static_cat > 0 else [1]\n",
    "        )\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.scaling = scaling\n",
    "        self.lags_seq = lags_seq\n",
    "        self.time_features = (\n",
    "            time_features\n",
    "            if time_features is not None\n",
    "            else time_features_from_frequency_str(self.freq)\n",
    "        )\n",
    "\n",
    "        self.num_parallel_samples = num_parallel_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches_per_epoch = num_batches_per_epoch\n",
    "\n",
    "        self.train_sampler = ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=prediction_length\n",
    "        )\n",
    "        self.validation_sampler = ValidationSplitSampler(\n",
    "            min_future=prediction_length\n",
    "        )\n",
    "\n",
    "    def create_transformation(self) -> Transformation:\n",
    "        remove_field_names = []\n",
    "        if self.num_feat_static_real == 0:\n",
    "            remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "        if self.num_feat_dynamic_real == 0:\n",
    "            remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "\n",
    "        return Chain(\n",
    "            [RemoveFields(field_names=remove_field_names)]\n",
    "            + (\n",
    "                [SetField(output_field=FieldName.FEAT_STATIC_CAT, value=[0])]\n",
    "                if not self.num_feat_static_cat > 0\n",
    "                else []\n",
    "            )\n",
    "            + (\n",
    "                [\n",
    "                    SetField(\n",
    "                        output_field=FieldName.FEAT_STATIC_REAL, value=[0.0]\n",
    "                    )\n",
    "                ]\n",
    "                if not self.num_feat_static_real > 0\n",
    "                else []\n",
    "            )\n",
    "            + [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                ),\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                ),\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.TARGET,\n",
    "                    # in the following line, we add 1 for the time dimension\n",
    "                    expected_ndim=1 + len(self.distr_output.event_shape),\n",
    "                ),\n",
    "                AddObservedValuesIndicator(\n",
    "                    target_field=FieldName.TARGET,\n",
    "                    output_field=FieldName.OBSERVED_VALUES,\n",
    "                ),\n",
    "                AddTimeFeatures(\n",
    "                    start_field=FieldName.START,\n",
    "                    target_field=FieldName.TARGET,\n",
    "                    output_field=FieldName.FEAT_TIME,\n",
    "                    time_features=self.time_features,\n",
    "                    pred_length=self.prediction_length,\n",
    "                ),\n",
    "                AddAgeFeature(\n",
    "                    target_field=FieldName.TARGET,\n",
    "                    output_field=FieldName.FEAT_AGE,\n",
    "                    pred_length=self.prediction_length,\n",
    "                    log_scale=True,\n",
    "                ),\n",
    "                VstackFeatures(\n",
    "                    output_field=FieldName.FEAT_TIME,\n",
    "                    input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                    + (\n",
    "                        [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                        if self.num_feat_dynamic_real > 0\n",
    "                        else []\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_instance_splitter(\n",
    "        self, module: S4LightningModule, mode: str\n",
    "    ):\n",
    "        assert mode in [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        instance_sampler = {\n",
    "            \"training\": self.train_sampler,\n",
    "            \"validation\": self.validation_sampler,\n",
    "            \"test\": TestSplitSampler(),\n",
    "        }[mode]\n",
    "\n",
    "        return InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            instance_sampler=instance_sampler,\n",
    "            past_length=module.model._past_length,\n",
    "            future_length=self.prediction_length,\n",
    "            time_series_fields=[\n",
    "                FieldName.FEAT_TIME,\n",
    "                FieldName.OBSERVED_VALUES,\n",
    "            ],\n",
    "            dummy_value=self.distr_output.value_in_support,\n",
    "        )\n",
    "\n",
    "    def create_training_data_loader(\n",
    "        self,\n",
    "        data: Dataset,\n",
    "        module: S4LightningModule,\n",
    "        shuffle_buffer_length: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ) -> Iterable:\n",
    "        data = Cyclic(data).stream()\n",
    "        instances = self._create_instance_splitter(module, \"training\").apply(\n",
    "            data, is_train=True\n",
    "        )\n",
    "        return as_stacked_batches(\n",
    "            instances,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "            field_names=TRAINING_INPUT_NAMES,\n",
    "            output_type=torch.tensor,\n",
    "            num_batches_per_epoch=self.num_batches_per_epoch,\n",
    "        )\n",
    "\n",
    "    def create_validation_data_loader(\n",
    "        self,\n",
    "        data: Dataset,\n",
    "        module: S4LightningModule,\n",
    "        **kwargs,\n",
    "    ) -> Iterable:\n",
    "        instances = self._create_instance_splitter(module, \"validation\").apply(\n",
    "            data, is_train=True\n",
    "        )\n",
    "        return as_stacked_batches(\n",
    "            instances,\n",
    "            batch_size=self.batch_size,\n",
    "            field_names=TRAINING_INPUT_NAMES,\n",
    "            output_type=torch.tensor,\n",
    "        )\n",
    "\n",
    "    def create_lightning_module(self) -> S4LightningModule:\n",
    "        model = S4Model(\n",
    "            freq=self.freq,\n",
    "            context_length=self.context_length,\n",
    "            prediction_length=self.prediction_length,\n",
    "            num_feat_dynamic_real=(\n",
    "                1 + self.num_feat_dynamic_real + len(self.time_features)\n",
    "            ),\n",
    "            num_feat_static_real=max(1, self.num_feat_static_real),\n",
    "            num_feat_static_cat=max(1, self.num_feat_static_cat),\n",
    "            cardinality=self.cardinality,\n",
    "            embedding_dimension=self.embedding_dimension,\n",
    "            \n",
    "            d_state=self.d_state,\n",
    "            num_layers=self.num_layers,\n",
    "            nhead=self.nhead,\n",
    "            activation=self.activation,\n",
    "            prenorm=self.prenorm,\n",
    "            postact=self.postact,\n",
    "            measure=self.measure,\n",
    "            trainable=self.trainable,\n",
    "            \n",
    "            distr_output=self.distr_output,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            lags_seq=self.lags_seq,\n",
    "            scaling=self.scaling,\n",
    "            num_parallel_samples=self.num_parallel_samples,\n",
    "        )\n",
    "\n",
    "        return S4LightningModule(model=model, loss=self.loss)\n",
    "\n",
    "    def create_predictor(\n",
    "        self,\n",
    "        transformation: Transformation,\n",
    "        module: S4LightningModule,\n",
    "    ) -> PyTorchPredictor:\n",
    "        prediction_splitter = self._create_instance_splitter(module, \"test\")\n",
    "\n",
    "        return PyTorchPredictor(\n",
    "            input_transform=transformation + prediction_splitter,\n",
    "            input_names=PREDICTION_INPUT_NAMES,\n",
    "            prediction_net=module.model,\n",
    "            batch_size=self.batch_size,\n",
    "            prediction_length=self.prediction_length,\n",
    "            device=torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e536f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 261MB [01:12, 3.62MB/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qn/nd7ywmt97gb_r8h3mg8q8vpc0000gn/T/ipykernel_4337/790282162.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electricity_load_diagrams\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstnet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/venvs/pytsvenv/lib/python3.10/site-packages/datasets/load.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0;31m# Don't try downloading from Google storage for the packaged datasets as text, json, csv or pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m     \u001b[0mtry_from_hf_gcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PACKAGED_DATASETS_MODULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m         \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/venvs/pytsvenv/lib/python3.10/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mmax_shard_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                             \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_shard_size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                             \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_proc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                         self._download_and_prepare(\n\u001b[0m\u001b[1;32m    955\u001b[0m                             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                             \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/venvs/pytsvenv/lib/python3.10/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m         super()._download_and_prepare(\n\u001b[0m\u001b[1;32m   1718\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m             \u001b[0mcheck_duplicate_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASIC_CHECKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/venvs/pytsvenv/lib/python3.10/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \"\"\"\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# Generating data for all splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0msplit_generators_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_split_generators_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0msplit_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msplit_generators_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# Checksums verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverification_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_CHECKS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_checksums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/electricity_load_diagrams/fe3dd01c39428ad92523a7ced0df3fdf669cb0548b3dd16fb9f7009381aa440f/electricity_load_diagrams.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mval_end_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# training ends at 6/10-th of the time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mtrain_end_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mts_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0msliced_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_end_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/venvs/pytsvenv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"electricity_load_diagrams\", \"lstnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1H\"\n",
    "prediction_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ListDataset(dataset[\"train\"], freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ListDataset(dataset[\"validation\"], freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63298518",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = S4Estimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=10*prediction_length,\n",
    "    measure=\"legs\",\n",
    "    trainable={\"dt\": True, \"A\":True, \"P\": True, \"B\": True},\n",
    "    \n",
    "    nhead=2,\n",
    "    num_layers=2,\n",
    "    \n",
    "    batch_size=128,\n",
    "    num_batches_per_epoch=100,\n",
    "    trainer_kwargs=dict(max_epochs=20, gpus='1', precision=\"bf16\", logger=CSVLogger(\"logs\", name=\"transformer\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(\n",
    "    training_data=train_ds,\n",
    "    validation_data=val_ds,\n",
    "    num_workers=8,\n",
    "    shuffle_buffer_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566aa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ListDataset(dataset[\"test\"], freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eae2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,\n",
    "    predictor=predictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee452cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(forecast_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b85112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be971a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c10cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    ts[-4 * prediction_length:].plot(ax=ax, label=\"target\",)\n",
    "    forecast.plot(color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e9fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
